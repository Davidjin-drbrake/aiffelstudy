{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "another-nudist",
   "metadata": {},
   "source": [
    "# E6 인공지능 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-italian",
   "metadata": {},
   "source": [
    "## ?? 프로그램 목적 : 인공지능으로 멋진 작사 해 보기\n",
    "            \n",
    "### (정규표현식 - 데이터 다듬기, Embadding, LSTM으로 학습 시키기, 잘 만들어 졌는지 평가하기)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-paris",
   "metadata": {},
   "source": [
    "### Step 1,2. 데이터 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "american-ebony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj29/lyricist/data/lyrics/dr-seuss.txt\n",
      "데이터 크기: 1005\n",
      "Examples:\n",
      " ['The Cat in the Hat', 'By Dr. Seuss', 'The sun did not shine.', 'It was too wet to play.', 'So we sat in the house']\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/eminem.txt\n",
      "데이터 크기: 6812\n",
      "Examples:\n",
      " ['Look, I was gonna go easy on you and not to hurt your feelings', \"But I'm only going to get this one chance\", \"Something's wrong, I can feel it (Six minutes, Slim Shady, you're on)\", \"Just a feeling I've got, like something's about to happen, but I don't know what\", \"If that means, what I think it means, we're in trouble, big trouble,\"]\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/drake.txt\n",
      "데이터 크기: 4773\n",
      "Examples:\n",
      " ['[Hook]', \"I've been down so long, it look like up to me\", 'They look up to me', \"I got fake people showin' fake love to me\", 'Straight up to my face, straight up to my face']\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/al-green.txt\n",
      "데이터 크기: 2130\n",
      "Examples:\n",
      " [\"Let's stay together I, I'm I'm so in love with you\", 'Whatever you want to do', 'Is all right with me', 'Cause you make me feel so brand new', \"And I want to spend my life with you Let me say that since, baby, since we've been together\"]\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/nicki-minaj.txt\n",
      "데이터 크기: 4926\n",
      "Examples:\n",
      " ['[:Nicki Minaj]', 'Young money [Verse 1: Jason Derulo] (?) thousand different favors', 'I wish that I could (?)', \"No i ain't got no dinner plans\", 'So you should bring all your friends']\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/lil-wayne.txt\n",
      "데이터 크기: 3155\n",
      "Examples:\n",
      " ['I bought my first key from my baby momma brother', 'I bought my first key', 'Bought my bought my first key', 'I bought my first key from my baby momma brother', 'I bought my first key']\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/notorious-big.txt\n",
      "데이터 크기: 5283\n",
      "Examples:\n",
      " ['\\ufeffbaby It was all a dream', 'I used to read Word Up magazine', 'Salt n Pepa and Heavy D up in the limousine', 'Hangin pictures on my wall', 'Every Saturday Rap Attack Mr Magic Marley Marl']\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/dolly-parton.txt\n",
      "데이터 크기: 1939\n",
      "Examples:\n",
      " ['Jolene, Jolene, Jolene, Jolene', \"I'm begging of you please don't take my man\", 'Jolene, Jolene, Jolene, Jolene', \"Please don't take him just because you can Your beauty is beyond compare\", 'With flaming locks of auburn hair']\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/adele.txt\n",
      "데이터 크기: 2400\n",
      "Examples:\n",
      " ['Looking for some education', 'Made my way into the night', 'All that bullshit conversation', \"Baby, can't you read the signs? I won't bore you with the details, baby\", \"I don't even wanna waste your time\"]\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/kanye-west.txt\n",
      "데이터 크기: 6192\n",
      "Examples:\n",
      " ['\\ufeffLet the suicide doors up', 'I threw suicides on the tour bus', 'I threw suicides on the private jet', 'You know what that mean Im fly to death', 'I step in Def Jam buildin like Im the shit']\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/notorious_big.txt\n",
      "데이터 크기: 5282\n",
      "Examples:\n",
      " ['\\ufeffbaby It was all a dream', 'I used to read Word Up magazine', 'Salt n Pepa and Heavy D up in the limousine', 'Hangin pictures on my wall', 'Every Saturday Rap Attack Mr Magic Marley Marl']\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/Kanye_West.txt\n",
      "데이터 크기: 6192\n",
      "Examples:\n",
      " ['\\ufeffEighteen years eighteen years', 'She got one of your kids got you for eighteen years', 'I know somebody paying child support for one of his kids', 'His baby mama car and crib is bigger than his', 'You will see him on TV any given Sunday']\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/patti-smith.txt\n",
      "데이터 크기: 3056\n",
      "Examples:\n",
      " ['', '', \"Jesus died for somebody's sins but not mine\", \"Meltin' in a pot of thieves\", 'Wild card up my sleeve']\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/paul-simon.txt\n",
      "데이터 크기: 2247\n",
      "Examples:\n",
      " ['Hey, Vietnam, Vietnam, Vietnam, Vietnam', 'Vietnam, Vietnam, Vietnam Yesterday I got a letter from my friend', 'Fighting in Vietnam', 'And this is what he had to say', \"'Tell all my friends that I'll be coming home soon\"]\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/ludacris.txt\n",
      "데이터 크기: 5029\n",
      "Examples:\n",
      " ['Aw yeah, yeah', \"I mean there's a lot of women out here that's just, you know\", 'They just defensive in so many things, man', 'You missing so many key nutrients in your life', 'I, I just want you to stay healthy in these streets']\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/bieber.txt\n",
      "데이터 크기: 3715\n",
      "Examples:\n",
      " ['What do you mean?', 'Oh, oh, oh', 'When you sometimes say yes', 'But you sometimes say no', 'What do you mean?']\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/radiohead.txt\n",
      "데이터 크기: 2346\n",
      "Examples:\n",
      " ['Come on, come on', 'You think you drive me crazy', 'Come on, come on', 'You and whose army?', 'You and your cronies']\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/nickelback.txt\n",
      "데이터 크기: 3425\n",
      "Examples:\n",
      " ['The first words that come out', 'And I can see this song will be about you', \"I can't believe that I can breathe without you\", 'But all I need to do is carry on', 'The next line I write down']\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/beatles.txt\n",
      "데이터 크기: 1846\n",
      "Examples:\n",
      " ['Yesterday, all my troubles seemed so far away', \"Now it looks as though they're here to stay\", \"Oh, I believe in yesterday Suddenly, I'm not half the man I used to be\", \"There's a shadow hanging over me.\", \"Oh, yesterday came suddenly Why she had to go I don't know she wouldn't say\"]\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/rihanna.txt\n",
      "데이터 크기: 3895\n",
      "Examples:\n",
      " ['Ghost in the mirror', \"I knew your face once, but now it's unclear\", \"And I can't feel my body now\", \"I'm separate from here and now A drug and a dream\", 'We lost connection, oh come back to me']\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/johnny-cash.txt\n",
      "데이터 크기: 1935\n",
      "Examples:\n",
      " ['I saw you walking by his side heard you whisper all those lies', \"And I couldn't keep from crying\", 'You sang him love songs tenderly that should have been for you and me', \"And I couldn't keep from crying\", 'I saw his eyes drinking your charms while he held you in his arms']\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/lady-gaga.txt\n",
      "데이터 크기: 3807\n",
      "Examples:\n",
      " [\"I'll undress you, 'cause you're tired\", 'Cover you as you desire', 'When you fall asleep inside my arms', 'May not have the fancy things', \"But I'll give you everything\"]\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/britney-spears.txt\n",
      "데이터 크기: 3848\n",
      "Examples:\n",
      " ['They say get ready for the revolution', \"I think it's time we find some sorta solution\", \"Somebody's caught up in the endless pollution\", 'They need to wake up, stop living illusions I know you need to hear this', \"Why won't somebody feel this\"]\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/bob-marley.txt\n",
      "데이터 크기: 2218\n",
      "Examples:\n",
      " ['\"Don\\'t worry about a thing,', \"'Cause every little thing gonna be all right.\", 'Singin\\': \"Don\\'t worry about a thing,', '\\'Cause every little thing gonna be all right!\" Rise up this mornin\\',', \"Smiled with the risin' sun,\"]\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/bruce-springsteen.txt\n",
      "데이터 크기: 2413\n",
      "Examples:\n",
      " ['[Verse 1]', 'They come from everywhere', 'A longing to be free', 'They come to join us here', 'From sea to shining sea And they all have a dream']\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/michael-jackson.txt\n",
      "데이터 크기: 11176\n",
      "Examples:\n",
      " ['', '', '[Spoken Intro:]', 'You ever want something ', \"that you know you shouldn't have \"]\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/missy-elliott.txt\n",
      "데이터 크기: 5056\n",
      "Examples:\n",
      " [\"I'mma start it from the bottom\", \"I'll show you how to flip a dollar\", 'I got food in my dining room', \"I'm better, I'm better, I'm better\", \"It's another day, another chance\"]\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/kanye.txt\n",
      "데이터 크기: 3799\n",
      "Examples:\n",
      " ['', \"N-now th-that that don't kill me\", 'Can only make me stronger', 'I need you to hurry up now', \"Cause I can't wait much longer\"]\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/bruno-mars.txt\n",
      "데이터 크기: 3270\n",
      "Examples:\n",
      " ['Now greetings to the world! Standing at this liquor store,', 'Whiskey coming through my pores,', 'Feeling like I run this whole block.', 'Lotto tickets cheap beer', \"That's why you can catch me here,\"]\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/bjork.txt\n",
      "데이터 크기: 1693\n",
      "Examples:\n",
      " ['If you ever get close to a human and human behavior', 'Be ready, be ready to get confused and me and my here after', \"There's definitely, definitely, definitely no logic to human behavior\", 'But yet so, yet so irresistible and me and my fear can', \"And there is no map uncertain They're terribly, terribly, terribly moody of human behavior\"]\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/dj-khaled.txt\n",
      "데이터 크기: 5717\n",
      "Examples:\n",
      " ['Another one', 'We The Best music', \"DJ Khaled I don't know if you could take it\", 'Know you wanna see me nakey, nakey, naked', 'I wanna be your baby, baby, baby']\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/Lil_Wayne.txt\n",
      "데이터 크기: 3155\n",
      "Examples:\n",
      " ['\\ufeffThey call me Mr Carter I kissed the daughter', 'Of the deads forehead I killed the father', 'Spilled the heart of a mildew hater', 'I will put them body on chill like glaciers', 'Gracias Im crazy yes its obvious']\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/amy-winehouse.txt\n",
      "데이터 크기: 1910\n",
      "Examples:\n",
      " ['Build your dreams to the stars above', 'But when you need someone to love', \"Don't go to strangers, darling, come to me Play with fire till your fingers burn\", \"And when there's no place for you to turn\", \"Don't go to strangers, darling, come to me For, when you hear a call to follow your heart\"]\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/r-kelly.txt\n",
      "데이터 크기: 4123\n",
      "Examples:\n",
      " ['I hear you callin\\', \"Here I come baby\"', 'To save you, oh oh', \"Baby no more stallin'\", 'These hands have been longing to touch you baby', \"And now that you've come around, to seein' it my way\"]\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/lorde.txt\n",
      "데이터 크기: 1671\n",
      "Examples:\n",
      " ['Well, summer slipped us underneath her tongue', 'Our days and nights are perfumed with obsession', 'Half of my wardrobe is on your bedroom floor', 'Use our eyes, throw our hands overboard I am your sweetheart psychopathic crush', \"Drink up your movements, still I can't get enough\"]\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/leonard-cohen.txt\n",
      "데이터 크기: 2584\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\", 'It goes like this', 'The fourth, the fifth']\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/nirvana.txt\n",
      "데이터 크기: 1985\n",
      "Examples:\n",
      " ['Load up on guns, bring your friends', \"It's fun to lose and to pretend\", \"She's over-bored and self-assured\", 'Oh no, I know a dirty word Hello, hello, hello, how low', 'Hello, hello, hello, how low']\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/joni-mitchell.txt\n",
      "데이터 크기: 2976\n",
      "Examples:\n",
      " ['Just before our love got lost you said', 'I am as constant as a northern star\" and I said', 'Constantly in the darkness', \"Where's that at\", \"If you want me I'll be in the bar On the back of a cartoon coaster\"]\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/jimi-hendrix.txt\n",
      "데이터 크기: 2033\n",
      "Examples:\n",
      " [' There must be some kind of way outta here', 'Said the joker to the thief', \"There's too much confusion\", \"I can't get no relief Business men, they drink my wine\", 'Plowman dig my earth']\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/cake.txt\n",
      "데이터 크기: 2023\n",
      "Examples:\n",
      " ['At first I was afraid', 'I was petrified', 'I kept thinking I could never live without you', 'By my side But then I spent so many nights', \"Just thinking how you've done me wrong\"]\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/bob-dylan.txt\n",
      "데이터 크기: 6038\n",
      "Examples:\n",
      " ['', '\\t\\t\\t“There must be some way out of here,” said the joker to the thief', '“There’s too much confusion, I can’t get no relief', 'Businessmen, they drink my wine, plowmen dig my earth', 'None of them along the line know what any of it is worth”']\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/alicia-keys.txt\n",
      "데이터 크기: 2897\n",
      "Examples:\n",
      " ['Ooh....... New York x2 Grew up in a town that is famous as a place of movie scenes', 'Noise is always loud, there are sirens all around and the streets are mean', \"If I can make it here, I can make it anywhere, that's what they say\", \"Seeing my face in lights or my name on marquees found down on Broadway Even if it ain't all it seems, I got a pocket full of dreams\", \"Baby, I'm from New York\"]\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/nursery_rhymes.txt\n",
      "데이터 크기: 5070\n",
      "Examples:\n",
      " ['THE QUEEN _of_ HEARTS', '', '', '    The Queen of Hearts she made some tarts,', \"      All on a summer's day;\"]\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/prince.txt\n",
      "데이터 크기: 11749\n",
      "Examples:\n",
      " ['', '', 'All of this and more is for you', 'With love, sincerity and deepest care', 'My life with you I share']\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/blink-182.txt\n",
      "데이터 크기: 2332\n",
      "Examples:\n",
      " ['Can we forget about the things I said when I was drunk...', \"I didn't mean to call you that\", \"I can't remember what was said\", 'Or what you threw at me Please tell me', 'Please tell me why']\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/lin-manuel-miranda.txt\n",
      "데이터 크기: 1084\n",
      "Examples:\n",
      " ['How does a bastard, orphan, son of a whore', 'And a Scotsman, dropped in the middle of a forgotten spot in the Caribbean by providence impoverished,', 'In squalor, grow up to be a hero and a scholar? The ten-dollar founding father without a father', 'Got a lot farther by working a lot harder', 'By being a lot smarter By being a self-starter']\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/dickinson.txt\n",
      "데이터 크기: 10053\n",
      "Examples:\n",
      " ['I. LIFE.', '', '', '', '']\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/disney.txt\n",
      "데이터 크기: 2499\n",
      "Examples:\n",
      " ['When somebody loved me', 'Everything was beautiful', 'Every hour we spent together', 'Lives within my heart And when she was sad', 'I was there to dry her tears']\n",
      "\n",
      "\n",
      "/home/aiffel-dj29/lyricist/data/lyrics/janisjoplin.txt\n",
      "데이터 크기: 2326\n",
      "Examples:\n",
      " [\"Busted flat in Baton Rouge, waitin' for a train\", \"And I's feelin' near as faded as my jeans\", 'Bobby thumbed a diesel down, just before it rained', 'It rode us all the way to New Orleans I pulled my harpoon out of my dirty red bandanna', \"I was playin' soft while Bobby sang the blues, yeah\"]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split # 데이터셋 분리 \n",
    "\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/lyricist/data/lyrics/*'\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        print(txt_file)\n",
    "        print(\"데이터 크기:\", len(raw))\n",
    "        print(\"Examples:\\n\", raw[:5])\n",
    "        print(\"\\n\")\n",
    "        raw_corpus.extend(raw)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-history",
   "metadata": {},
   "source": [
    "## !! 폴더에 들어 있는 각 파일의 형태를 보고 싶어서 5개씩 출력 해 보니 제거해야할 부분이 보인다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-range",
   "metadata": {},
   "source": [
    "## Step 3. 데이터 정제\n",
    "### Key point : preprocess_sentence() 함수 사용, 지나치게 긴 문장은  제거,  토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습데이터에서 제외하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "right-conflict",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제외 전 문장수 :  187088\n",
      "제외 후 문장수 :  174176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<start> the cat in the hat <end>',\n",
       " '<start> by dr . seuss <end>',\n",
       " '<start> the sun did not shine . <end>',\n",
       " '<start> it was too wet to play . <end>',\n",
       " '<start> so we sat in the house <end>',\n",
       " '<start> all that cold cold wet day . <end>',\n",
       " '<start> i sat there with sally . <end>',\n",
       " '<start> we sat there we two . <end>',\n",
       " '<start> and i said how i wish <end>',\n",
       " '<start> we had something to do ! <end>']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)    # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)           # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    return sentence\n",
    "\n",
    "# 문장길이가 100이상으로 너무 길면 제외\n",
    "corpus = []\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if len(sentence) > 100: continue   # 길이가 100인 문장은 건너뜁니다. 토큰 15개와 비슷한 길이\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "\n",
    "# 10개 정도 출력 해보자\n",
    "print(\"제외 전 문장수 : \", len(raw_corpus))\n",
    "print(\"제외 후 문장수 : \", len(corpus))\n",
    "\n",
    "corpus[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-chain",
   "metadata": {},
   "source": [
    "### 자 이제 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "amazing-begin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2    6  900 ...    0    0    0]\n",
      " [   2  120 2638 ...    0    0    0]\n",
      " [   2    6  300 ...    0    0    0]\n",
      " ...\n",
      " [   2  135    4 ...    0    0    0]\n",
      " [   2  122  169 ...    0    0    0]\n",
      " [   2  135    4 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fbb5b652f10>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,  # 전체 단어의 개수 \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus) \n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-theta",
   "metadata": {},
   "source": [
    "## Step 4. 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-evidence",
   "metadata": {},
   "source": [
    "### 데이터셋 분리를 위한 텐서 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "representative-sleeping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2    6  900   14    6 1335    3    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   6  900   14    6 1335    3    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]   # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성 마지막 토큰은 <end>가 아니라 <pad>일 가능성\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-spare",
   "metadata": {},
   "source": [
    "### 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "knowing-young",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENC Train:  (139340, 34)\n",
      "DEC Train:  (139340, 34)\n",
      "ENC Test:  (34836, 34)\n",
      "DEC Test:  (34836, 34)\n"
     ]
    }
   ],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,\n",
    "                                                  tgt_input,\n",
    "                                                  test_size = 0.2,  # 총 데이터의 20%\n",
    "                                                  shuffle = True\n",
    "                                                 )\n",
    "\n",
    "print(\"ENC Train: \", enc_train.shape)\n",
    "print(\"DEC Train: \", dec_train.shape)\n",
    "print(\"ENC Test: \", enc_val.shape)\n",
    "print(\"DEC Test: \", dec_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-schedule",
   "metadata": {},
   "source": [
    "### 하이퍼 파라메타 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "atlantic-verification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffer size :  174176\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "#STEPS_PER_EPOCH = len(enc_train) // BATCH_SIZE\n",
    "#VALIDATION_STEPS = len(enc_val) // BATCH_SIZE\n",
    "print(\"buffer size : \", BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sealed-surveillance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<BatchDataset shapes: ((256, 34), (256, 34)), types: (tf.int32, tf.int32)>,\n",
       " <BatchDataset shapes: ((256, 34), (256, 34)), types: (tf.int32, tf.int32)>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 pad>를 포함하여 7001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    \n",
    "\n",
    "#dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "#dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "#dataset\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).shuffle(BUFFER_SIZE)\n",
    "train_data = train_data.batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_data = tf.data.Dataset.from_tensor_slices((enc_val, dec_val)).shuffle(BUFFER_SIZE)\n",
    "test_data = test_data.batch(BATCH_SIZE, drop_remainder=True)\n",
    "train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-toner",
   "metadata": {},
   "source": [
    "## Step 5. 인공지능 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-cradle",
   "metadata": {},
   "source": [
    "### 모델 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "accessible-approval",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        # 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔줍니다. \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "vocab_size = VOCAB_SIZE\n",
    "embedding_size = 256  \n",
    "hidden_size = 1024\n",
    "model = TextGenerator(vocab_size, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "novel-absence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "thick-fishing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 34, 12001), dtype=float32, numpy=\n",
       "array([[[-7.7990626e-05,  3.7708956e-05, -8.3427964e-05, ...,\n",
       "          1.3166625e-04, -1.9439554e-04,  7.7856967e-05],\n",
       "        [-1.5485342e-04,  1.6902464e-04, -2.6384200e-04, ...,\n",
       "          1.6632148e-04, -2.8648489e-04, -7.9584912e-05],\n",
       "        [-2.6284379e-04,  2.1243877e-04, -3.4325288e-04, ...,\n",
       "          3.0686596e-04, -1.5607038e-04, -1.9492916e-04],\n",
       "        ...,\n",
       "        [-1.2399342e-03, -1.4065544e-03,  3.3599245e-03, ...,\n",
       "         -2.0883812e-03, -2.3864559e-03, -4.4734902e-03],\n",
       "        [-1.2829560e-03, -1.4388468e-03,  3.3879909e-03, ...,\n",
       "         -2.1410210e-03, -2.3958413e-03, -4.4807415e-03],\n",
       "        [-1.3188202e-03, -1.4653449e-03,  3.4092118e-03, ...,\n",
       "         -2.1863144e-03, -2.4027601e-03, -4.4859629e-03]],\n",
       "\n",
       "       [[-7.7990626e-05,  3.7708956e-05, -8.3427964e-05, ...,\n",
       "          1.3166625e-04, -1.9439554e-04,  7.7856967e-05],\n",
       "        [-4.8431815e-05, -5.1512485e-05,  1.0089142e-04, ...,\n",
       "          2.4484363e-04, -2.9789499e-04, -1.4403646e-04],\n",
       "        [-1.6633110e-04, -2.3762265e-04,  3.9066139e-04, ...,\n",
       "          6.2484579e-04, -4.0332609e-04, -2.7408035e-04],\n",
       "        ...,\n",
       "        [-1.4504815e-03, -1.4821220e-03,  3.4615521e-03, ...,\n",
       "         -2.3484468e-03, -2.4710933e-03, -4.4164187e-03],\n",
       "        [-1.4586905e-03, -1.4944654e-03,  3.4720623e-03, ...,\n",
       "         -2.3656175e-03, -2.4677075e-03, -4.4336733e-03],\n",
       "        [-1.4648616e-03, -1.5050516e-03,  3.4790325e-03, ...,\n",
       "         -2.3797711e-03, -2.4646507e-03, -4.4476581e-03]],\n",
       "\n",
       "       [[-7.7990626e-05,  3.7708956e-05, -8.3427964e-05, ...,\n",
       "          1.3166625e-04, -1.9439554e-04,  7.7856967e-05],\n",
       "        [ 6.4583299e-05,  2.6634763e-04,  3.9575439e-06, ...,\n",
       "         -4.7211375e-05,  9.4196570e-05, -1.8526706e-06],\n",
       "        [ 1.6383853e-04,  5.0827285e-04, -3.1652056e-05, ...,\n",
       "         -1.2130598e-04,  2.0942847e-04,  5.6884965e-05],\n",
       "        ...,\n",
       "        [-1.4608522e-03, -1.5295293e-03,  3.4186346e-03, ...,\n",
       "         -2.3985323e-03, -2.4976640e-03, -4.4482495e-03],\n",
       "        [-1.4661498e-03, -1.5348394e-03,  3.4273171e-03, ...,\n",
       "         -2.4062120e-03, -2.4916360e-03, -4.4572838e-03],\n",
       "        [-1.4703122e-03, -1.5395428e-03,  3.4341058e-03, ...,\n",
       "         -2.4122787e-03, -2.4864043e-03, -4.4648536e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-7.7990626e-05,  3.7708956e-05, -8.3427964e-05, ...,\n",
       "          1.3166625e-04, -1.9439554e-04,  7.7856967e-05],\n",
       "        [-1.9310898e-04,  1.7736772e-04, -3.0269182e-05, ...,\n",
       "          3.9089189e-04, -8.2687584e-05,  4.3950105e-04],\n",
       "        [-2.8132432e-04,  2.6290034e-04,  1.1319224e-04, ...,\n",
       "          2.6001214e-04,  6.5694760e-05,  2.3343557e-04],\n",
       "        ...,\n",
       "        [-9.4871951e-04, -1.2517361e-03,  2.9774175e-03, ...,\n",
       "         -1.7378145e-03, -2.4610758e-03, -4.2365729e-03],\n",
       "        [-1.0386560e-03, -1.2856090e-03,  3.0631025e-03, ...,\n",
       "         -1.8292662e-03, -2.4763932e-03, -4.3165437e-03],\n",
       "        [-1.1147597e-03, -1.3177490e-03,  3.1335976e-03, ...,\n",
       "         -1.9128115e-03, -2.4825814e-03, -4.3770485e-03]],\n",
       "\n",
       "       [[-7.7990626e-05,  3.7708956e-05, -8.3427964e-05, ...,\n",
       "          1.3166625e-04, -1.9439554e-04,  7.7856967e-05],\n",
       "        [-6.2817591e-05,  1.1109417e-04, -2.2804401e-04, ...,\n",
       "          5.3885276e-05, -3.9611064e-04,  2.0559036e-04],\n",
       "        [-1.2813963e-04,  1.5460914e-04, -1.3299716e-04, ...,\n",
       "         -2.0471007e-04, -3.8243821e-04, -1.6141523e-04],\n",
       "        ...,\n",
       "        [-1.3936016e-03, -1.4963406e-03,  3.4657340e-03, ...,\n",
       "         -2.3161776e-03, -2.4455281e-03, -4.5179478e-03],\n",
       "        [-1.4086134e-03, -1.5109426e-03,  3.4679631e-03, ...,\n",
       "         -2.3383612e-03, -2.4434943e-03, -4.5176796e-03],\n",
       "        [-1.4209131e-03, -1.5231074e-03,  3.4691570e-03, ...,\n",
       "         -2.3565507e-03, -2.4418889e-03, -4.5167101e-03]],\n",
       "\n",
       "       [[-7.7990626e-05,  3.7708956e-05, -8.3427964e-05, ...,\n",
       "          1.3166625e-04, -1.9439554e-04,  7.7856967e-05],\n",
       "        [-1.5415423e-04,  1.7048836e-04, -3.5412879e-05, ...,\n",
       "          4.8282633e-05, -1.4880230e-04, -1.6984117e-04],\n",
       "        [-2.7336768e-04,  3.1665346e-04,  4.8524744e-05, ...,\n",
       "         -1.5712781e-04, -2.2833048e-05, -5.4552214e-04],\n",
       "        ...,\n",
       "        [-1.4490697e-03, -1.3984825e-03,  3.4645367e-03, ...,\n",
       "         -2.2853296e-03, -2.5412792e-03, -4.3900302e-03],\n",
       "        [-1.4676322e-03, -1.4291337e-03,  3.4878980e-03, ...,\n",
       "         -2.3165285e-03, -2.5388098e-03, -4.4120639e-03],\n",
       "        [-1.4812053e-03, -1.4543142e-03,  3.5033494e-03, ...,\n",
       "         -2.3421294e-03, -2.5334847e-03, -4.4291141e-03]]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in train_data.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-spelling",
   "metadata": {},
   "source": [
    "### 모델 학습 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "roman-chinese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "544/544 [==============================] - 160s 293ms/step - loss: 1.6688 - val_loss: 1.4410\n",
      "Epoch 2/30\n",
      "544/544 [==============================] - 160s 293ms/step - loss: 1.3900 - val_loss: 1.3602\n",
      "Epoch 3/30\n",
      "544/544 [==============================] - 160s 294ms/step - loss: 1.3170 - val_loss: 1.3075\n",
      "Epoch 4/30\n",
      "544/544 [==============================] - 161s 296ms/step - loss: 1.2609 - val_loss: 1.2698\n",
      "Epoch 5/30\n",
      "544/544 [==============================] - 162s 297ms/step - loss: 1.2137 - val_loss: 1.2399\n",
      "Epoch 6/30\n",
      "544/544 [==============================] - 162s 298ms/step - loss: 1.1712 - val_loss: 1.2134\n",
      "Epoch 7/30\n",
      "544/544 [==============================] - 163s 299ms/step - loss: 1.1319 - val_loss: 1.1918\n",
      "Epoch 8/30\n",
      "544/544 [==============================] - 163s 300ms/step - loss: 1.0946 - val_loss: 1.1739\n",
      "Epoch 9/30\n",
      "544/544 [==============================] - 164s 301ms/step - loss: 1.0592 - val_loss: 1.1578\n",
      "Epoch 10/30\n",
      "544/544 [==============================] - 165s 303ms/step - loss: 1.0257 - val_loss: 1.1445\n",
      "Epoch 11/30\n",
      "544/544 [==============================] - 166s 304ms/step - loss: 0.9936 - val_loss: 1.1323\n",
      "Epoch 12/30\n",
      "544/544 [==============================] - 166s 305ms/step - loss: 0.9633 - val_loss: 1.1221\n",
      "Epoch 13/30\n",
      "544/544 [==============================] - 167s 307ms/step - loss: 0.9342 - val_loss: 1.1148\n",
      "Epoch 14/30\n",
      "544/544 [==============================] - 167s 308ms/step - loss: 0.9061 - val_loss: 1.1082\n",
      "Epoch 15/30\n",
      "544/544 [==============================] - 168s 309ms/step - loss: 0.8797 - val_loss: 1.1030\n",
      "Epoch 16/30\n",
      "544/544 [==============================] - 168s 309ms/step - loss: 0.8543 - val_loss: 1.0986\n",
      "Epoch 17/30\n",
      "544/544 [==============================] - 169s 310ms/step - loss: 0.8299 - val_loss: 1.0953\n",
      "Epoch 18/30\n",
      "544/544 [==============================] - 169s 311ms/step - loss: 0.8068 - val_loss: 1.0941\n",
      "Epoch 19/30\n",
      "544/544 [==============================] - 169s 311ms/step - loss: 0.7846 - val_loss: 1.0933\n",
      "Epoch 20/30\n",
      "544/544 [==============================] - 170s 312ms/step - loss: 0.7633 - val_loss: 1.0920\n",
      "Epoch 21/30\n",
      "544/544 [==============================] - 170s 313ms/step - loss: 0.7429 - val_loss: 1.0928\n",
      "Epoch 22/30\n",
      "544/544 [==============================] - 170s 313ms/step - loss: 0.7231 - val_loss: 1.0941\n",
      "Epoch 23/30\n",
      "544/544 [==============================] - 171s 314ms/step - loss: 0.7041 - val_loss: 1.0958\n",
      "Epoch 24/30\n",
      "544/544 [==============================] - 171s 314ms/step - loss: 0.6860 - val_loss: 1.0980\n",
      "Epoch 25/30\n",
      "544/544 [==============================] - 171s 315ms/step - loss: 0.6684 - val_loss: 1.1007\n",
      "Epoch 26/30\n",
      "544/544 [==============================] - 172s 316ms/step - loss: 0.6517 - val_loss: 1.1046\n",
      "Epoch 27/30\n",
      "544/544 [==============================] - 172s 316ms/step - loss: 0.6355 - val_loss: 1.1075\n",
      "Epoch 28/30\n",
      "544/544 [==============================] - 172s 317ms/step - loss: 0.6201 - val_loss: 1.1135\n",
      "Epoch 29/30\n",
      "544/544 [==============================] - 173s 318ms/step - loss: 0.6054 - val_loss: 1.1157\n",
      "Epoch 30/30\n",
      "544/544 [==============================] - 173s 317ms/step - loss: 0.5912 - val_loss: 1.1222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbad2fdbc90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(train_data, validation_data = test_data, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-taylor",
   "metadata": {},
   "source": [
    "### Step.final : 모델 테스트 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "breeding-mobility",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "original-affairs",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love it when you call me big poppa <end> '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-junior",
   "metadata": {},
   "source": [
    "### 루브릭 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-cause",
   "metadata": {},
   "source": [
    "1. 가사 텍스트 생성 모델이 정상적으로 동작하는가?\n",
    "\t텍스트 제너레이션 결과가 그럴듯한 문장으로 생성되는가?\n",
    "    >> 전체적인 문장으로 결과가 나왔음.\n",
    "    \n",
    "2. 데이터의 전처리와 데이터셋 구성 과정이 체계적으로 진행되었는가?\n",
    "\t특수문자 제거, 토크나이저 생성, 패딩처리 등의 과정이 빠짐없이 진행되었는가?\n",
    "    >> 데이터 다듬기 완료 !!\n",
    "\n",
    "3. 텍스트 생성모델이 안정적으로 학습되었는가?\n",
    "\t텍스트 생성모델의 validation loss가 2.2 이하로 낮아졌는가?\n",
    "    >> val_loss 가 1.22로 2.2 이하 이다 음하하하"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-caution",
   "metadata": {},
   "source": [
    "### 느낀점과 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-morning",
   "metadata": {},
   "source": [
    " 1. 데이터 분리하는 방법이 split()으로 진행하여 dataset으로 연결하려는데 문제가 많았고 오래 걸렸음\n",
    " 2. 데이터 다듬는 방법도 아주 간격하게 잘 설명이 되어 있어서 향후 자주 사용할 것 같음.\n",
    " 3. 학습 모델으로 rnn을 처음 사용해 보았고 텍스트의 임베딩이 많아서 그런지 파라메터 값이 많이 나왔음. layer가 적지만.... "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
